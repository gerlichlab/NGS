"""A collections of functions to facilitate
analysis of HiC data based on the cooler and cooltools
interfaces."""
import multiprocess
import cooltools.expected
import cooltools.snipping
import pandas as pd
import bioframe
import cooler
import warnings
import pairtools
from typing import Tuple, Dict
import numpy as np


# define type aliases

cisTransPairs = Dict[str, pd.DataFrame]
pairsSamples = Dict[str, cisTransPairs]

# define functions


def getExpected(clr: cooler.Cooler, arms: pd.DataFrame,
                proc: int = 20, ignoreDiagonals: int = 2) -> pd.DataFrame:
    """Takes a clr file handle and a pandas dataframe
    with chromosomal arms (generated by getArmsHg19()) and calculates
    the expected read number at a certain genomic distance.
    The proc parameters defines how many processes should be used
    to do the calculations. ingore_diags specifies how many diagonals
    to ignore (0 mains the main diagonal, 1 means the main diagonal
    and the flanking tow diagonals and so on)"""
    with multiprocess.Pool(proc) as pool:
        expected = cooltools.expected.diagsum(
            clr,
            list(arms.itertuples(index=False, name=None)),
            transforms={
                'balanced': lambda p: p['count'] * p['weight1'] * p['weight2']
            },
            map=pool.map, ignore_diags=ignoreDiagonals
        )
    # construct a single dataframe for all regions (arms)
    expected_df = pd.concat([exp.reset_index().assign(chrom=reg[0], start=reg[1], end=reg[2])
                             for reg, exp in expected.items()])
    # aggregate diagonals over the regions specified by chrom, start, end (arms)
    expected_df = expected_df.groupby(['chrom', 'start', 'end', 'diag']).aggregate({
        'n_valid': 'sum',
        'count.sum': 'sum',
        'balanced.sum': 'sum'}).reset_index()
    # account for different number of valid bins in diagonals
    expected_df['balanced.avg'] = expected_df['balanced.sum'] / \
        expected_df['n_valid']
    return expected_df


def getArmsHg19() -> pd.DataFrame:
    """Downloads the coordinates for chromosomal arms of the
    genome assembly hg19 and returns it as a dataframe."""
    # download chromosomal sizes
    chromsizes = bioframe.fetch_chromsizes('hg19')
    # download centromers
    cens = bioframe.fetch_centromeres('hg19')
    cens.set_index('chrom', inplace=True)
    cens = cens.mid
    # define chromosomes that are well defined (filter out unassigned contigs)
    GOOD_CHROMS = list(chromsizes.index[:23])
    # construct arm regions (for each chromosome fro 0-centromere and from centromere to the end)
    arms = [arm for chrom in GOOD_CHROMS for arm in ((chrom, 0, cens.get(
        chrom, 0)), (chrom, cens.get(chrom, 0), chromsizes.get(chrom, 0)))]
    # construct dataframe out of arms
    arms = pd.DataFrame(arms, columns=['chrom', 'start', 'end'])
    return arms


def assignRegions(window: int, binsize: int, chroms: pd.Series,
                  positions: pd.Series, arms: pd.DataFrame) -> pd.DataFrame:
    """Constructs a 2d region around a series of chromosomal location.
    Window specifies the windowsize for the constructed regions. The total region
    assigned will be pos-window until pos+window. The binsize specifies the size
    of the HiC bins. The positions which represent the center of the regions
    is givin the the chroms series and the positions series."""
    # construct windows from the passed chromosomes and positions
    snipping_windows = cooltools.snipping.make_bin_aligned_windows(
        binsize,
        chroms.values,
        positions.values,
        window
    )
    # assign chromosomal arm to each position
    snipping_windows = cooltools.snipping.assign_regions(
        snipping_windows,
        list(arms.itertuples(index=False, name=None)))
    return snipping_windows


def doPileupObsExp(clr: cooler.Cooler, expected_df: pd.DataFrame,
                   snipping_windows: pd.DataFrame, proc: int = 5) -> np.ndarray:
    """Takes a cooler file handle, an expected dataframe
    constructed by getExpected, snipping windows constructed
    by assignRegions and performs a pileup on all these regions
    based on the obs/exp value. Returns a numpy array
    that contains averages of all selected regions."""
    oe_snipper = cooltools.snipping.ObsExpSnipper(clr, expected_df)
    # set warnings filter to ignore RuntimeWarnings since cooltools
    # does not check whether there are inf or 0 values in
    # the expected dataframe
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", RuntimeWarning)
        with multiprocess.Pool(proc) as pool:
            # extract a matrix of obs/exp avlues for each snipping_window
            oe_pile = cooltools.snipping.pileup(
                snipping_windows,
                oe_snipper.select, oe_snipper.snip,
                map=pool.map)
    # calculate the average of all windows
    collapsed_pile = np.nanmean(
        oe_pile[:, :, :], axis=2
    )
    return collapsed_pile


def slidingDiamond(array: np.ndarray, sideLen: int = 6) -> Tuple[np.ndarray, np.ndarray]:
    """Will slide a dimaond of side length 'sideLen'
    down the diagonal of the passed array and return
    the average values for each position and
    the relative position of each value with respect
    to the center of the array (in Bin units)"""
    # initialize accumulators for diamond value and x-position
    diamondAccumulator = list()
    binAccumulator = list()
    halfWindow = sideLen//2
    for i in range(0, (array.shape[0] - halfWindow)):
        # extract diamond
        diamondArray = array[i: (i+halfWindow) + 1, i:(i+halfWindow) + 1]
        # set inf to nan for calculation of mean
        diamondArray[np.isinf(diamondArray)] = np.nan
        diamondAccumulator.append(np.nanmean(diamondArray))
        # append x-value for this particular bin
        binAccumulator.append(np.mean(range(i, (i+halfWindow) + 1,)))
    return (np.array(binAccumulator - np.median(binAccumulator)), np.array(diamondAccumulator))


def loadPairs(path: str) -> pd.DataFrame:
    """Function to load a .pairs or .pairsam file
    into a pandas dataframe.
    This only works for relatively small files!"""
    # get handels for header and pairs_body
    header, pairs_body = pairtools._headerops.get_header(
            pairtools._fileio.auto_open(path, 'r'))
    # extract column names from header
    cols = pairtools._headerops.extract_column_names(header)
    # read data into dataframe
    frame = pd.read_csv(pairs_body, sep="\t", names=cols)
    return frame


def downSamplePairs(sampleDict: pairsSamples, Distance: int = 10**4) -> pairsSamples:
    """Will downsample cis and trans reads in sampleDict to contain
    as many combined cis and trans reads as the sample with the lowest readnumber of the
    specified distance. """
    # initialize output dictionary
    outDict = {sample: {} for sample in sampleDict}
    for sample in sampleDict.keys():
        # create temporary dataframes
        cisTemp = sampleDict[sample]["cis"]
        cisTemp["rType"] = "cis"
        transTemp = sampleDict[sample]["trans"]
        transTemp["rType"] = "trans"
        # concatenate them and store in outdict
        outDict[sample]["all"] = pd.concat((cisTemp, transTemp))
        # filter on distance
        outDict[sample]["all"] = outDict[sample]["all"].loc[(outDict[sample]["all"]["pos2"] - outDict[sample]["all"]["pos1"]) > Distance, :]
    # get the minimum number of reads
    minReads = min([len(i["all"]) for i in outDict.values()])
    # do the downsampling and split into cis and trans
    for sample in outDict.keys():
        outDict[sample]["all"] = outDict[sample]["all"].sample(n=minReads)
        outDict[sample]["cis"] = outDict[sample]["all"].loc[outDict[sample]
                                                            ["all"]["rType"] == "cis", :]
        outDict[sample]["trans"] = outDict[sample]["all"].loc[outDict[sample]
                                                              ["all"]["rType"] == "trans", :]
        # get rid of all reads
        outDict[sample].pop("all")
    return outDict
